{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2bda13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenization in /home/nextgen/anaconda3/envs/dnabert/lib/python3.6/site-packages (1.0.7)\r\n",
      "Requirement already satisfied: regex in /home/nextgen/anaconda3/envs/dnabert/lib/python3.6/site-packages (from tokenization) (2021.4.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b439e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_exit_code', '_i', '_i1', '_i2', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'quit']\n"
     ]
    }
   ],
   "source": [
    "print(dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d52f9271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nextgen/anaconda3/envs/dnabert/bin/python3: Error while finding module specification for 'src.transoformers.tokenization_utils' (ModuleNotFoundError: No module named 'src')\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m src.transoformers.tokenization_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccdf66e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "# 경로 문제 해결 (파일 dir 확인 후에 함수 패키지 불러오기)(https://m.blog.naver.com/wideeyed/221839634437)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b36a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__) #logging : 현재 우리의 프로그램이 어떤상태를 가지고 있는지 확인가능하다. ex) 결과로 나오는 ERROR:~~~ 여기에 나오는거 설정가능\n",
    "\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"bert-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\",\n",
    "        \"bert-large-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt\",\n",
    "        \"bert-base-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\",\n",
    "        \"bert-large-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt\",\n",
    "        \"bert-base-multilingual-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt\",\n",
    "        \"bert-base-multilingual-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt\",\n",
    "        \"bert-base-chinese\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt\",\n",
    "        \"bert-base-german-cased\": \"https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-vocab.txt\",\n",
    "        \"bert-large-uncased-whole-word-masking\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-vocab.txt\",\n",
    "        \"bert-large-cased-whole-word-masking\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-vocab.txt\",\n",
    "        \"bert-large-uncased-whole-word-masking-finetuned-squad\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-vocab.txt\",\n",
    "        \"bert-large-cased-whole-word-masking-finetuned-squad\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-vocab.txt\",\n",
    "        \"bert-base-cased-finetuned-mrpc\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-vocab.txt\",\n",
    "        \"bert-base-german-dbmdz-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt\",\n",
    "        \"bert-base-german-dbmdz-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-vocab.txt\",\n",
    "        \"bert-base-finnish-cased-v1\": \"https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-cased-v1/vocab.txt\",\n",
    "        \"bert-base-finnish-uncased-v1\": \"https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-uncased-v1/vocab.txt\",\n",
    "        \"bert-base-dutch-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/wietsedv/bert-base-dutch-cased/vocab.txt\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4857860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"bert-base-uncased\": 512,\n",
    "    \"bert-large-uncased\": 512,\n",
    "    \"bert-base-cased\": 512,\n",
    "    \"bert-large-cased\": 512,\n",
    "    \"bert-base-multilingual-uncased\": 512,\n",
    "    \"bert-base-multilingual-cased\": 512,\n",
    "    \"bert-base-chinese\": 512,\n",
    "    \"bert-base-german-cased\": 512,\n",
    "    \"bert-large-uncased-whole-word-masking\": 512,\n",
    "    \"bert-large-cased-whole-word-masking\": 512,\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\": 512,\n",
    "    \"bert-large-cased-whole-word-masking-finetuned-squad\": 512,\n",
    "    \"bert-base-cased-finetuned-mrpc\": 512,\n",
    "    \"bert-base-german-dbmdz-cased\": 512,\n",
    "    \"bert-base-german-dbmdz-uncased\": 512,\n",
    "    \"bert-base-finnish-cased-v1\": 512,\n",
    "    \"bert-base-finnish-uncased-v1\": 512,\n",
    "    \"bert-base-dutch-cased\": 512,\n",
    "}\n",
    "\n",
    "PRETRAINED_INIT_CONFIGURATION = {\n",
    "    \"bert-base-uncased\": {\"do_lower_case\": True}, #uncased(코퍼스가 모두 소문자로 구성) 모델 사용\n",
    "    \"bert-large-uncased\": {\"do_lower_case\": True}, \n",
    "    \"bert-base-cased\": {\"do_lower_case\": False}, #cased model 은 대소문자 구별가능한 모델\n",
    "    \"bert-large-cased\": {\"do_lower_case\": False},\n",
    "    \"bert-base-multilingual-uncased\": {\"do_lower_case\": True},\n",
    "    \"bert-base-multilingual-cased\": {\"do_lower_case\": False},\n",
    "    \"bert-base-chinese\": {\"do_lower_case\": False},\n",
    "    \"bert-base-german-cased\": {\"do_lower_case\": False},\n",
    "    \"bert-large-uncased-whole-word-masking\": {\"do_lower_case\": True},\n",
    "    \"bert-large-cased-whole-word-masking\": {\"do_lower_case\": False},\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\": {\"do_lower_case\": True},\n",
    "    \"bert-large-cased-whole-word-masking-finetuned-squad\": {\"do_lower_case\": False},\n",
    "    \"bert-base-cased-finetuned-mrpc\": {\"do_lower_case\": False},\n",
    "    \"bert-base-german-dbmdz-cased\": {\"do_lower_case\": False},\n",
    "    \"bert-base-german-dbmdz-uncased\": {\"do_lower_case\": True},\n",
    "    \"bert-base-finnish-cased-v1\": {\"do_lower_case\": False},\n",
    "    \"bert-base-finnish-uncased-v1\": {\"do_lower_case\": True},\n",
    "    \"bert-base-dutch-cased\": {\"do_lower_case\": False},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a3e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderDict() # 입력되는 값들의 입력 순서를 기억한다\n",
    "    with open(vocab_file, \"r\", encoding = \"utf-8\") as reader: #파일을 열고 닫는것을 자동적으로 처리\n",
    "        tokens = reader.readlines()\n",
    "    for index, token in enumerate(tokens):\n",
    "        token = token.rstrip(\"\\n\") #rstrip : 가장 오른쪽에 \\n(의미 : 줄바꿈) 제거하는 방법\n",
    "        vocab[token] = index\n",
    "    return vocab\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip() #왼쪽 오른쪽 둘다 제거\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3db5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizer(PreTrainedTokenizer):\n",
    "    r\"\"\"\n",
    "    Constructs a BertTokenizer.\n",
    "    구성\n",
    "    vocab_file : 단어 파일, do_lower_Case : input을 소문자로 변환, \n",
    "    do_basic_tokenize : wordpiece로 만들기 전에 토큰화 진행했는지\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        do_lower_case=True,\n",
    "        never_split=None,\n",
    "        unk_token=\"[UNK]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        tokenize_chinese_chars=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.max_len_single_sentence = self.max_len -2\n",
    "        self.max_len_sentences_pair = self.max_len -3\n",
    "        \n",
    "        if not os.path.isfile(vocab_file):\n",
    "            raise ValueError(\n",
    "                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained\"\n",
    "                \"model use 'tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)'\".format(vocab_file)\n",
    "            )\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in \n",
    "                            self.vocab.items()])\n",
    "        self.do_basic_tokenize = do_basic_tokenize\n",
    "        if do_basic_tokenize:\n",
    "            self.basic_tokenizer = BasicTokenizer(\n",
    "                do_lower_case=do_lower_case, never_split=never_split,\n",
    "            tokenize_chinese_chars=tokenize_chinese_chars\n",
    "            )\n",
    "            self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)\n",
    "            \n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        if self.do_basic_tokenize:\n",
    "            for token in self.basic_tokenizer.tokenize(text,\n",
    "                                                      never_split = self.all_special_tokens):\n",
    "                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "                    split_tokens.append(sub_token)\n",
    "        else:\n",
    "            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n",
    "        return split_tokens\n",
    "    \n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\"문자열(str)에서 id(??)로 바꾸기\"\"\"\n",
    "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "    \n",
    "    def _convert_token_to_token(self, index):\n",
    "        \"\"\"정수를 문자로 변환\"\"\"\n",
    "        return self.ids_to_tokens.get(index, self.unk_token)\n",
    "    \n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\"sequence 문자열에서 single 문자열로 변환\"\"\"\n",
    "        out_string = \" \".join(tokens).replace(\" ##\",\"\").strip()\n",
    "        return out_string\n",
    "    \n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\" Bert sequence 구성 \n",
    "        1) single : [CLS] X [SEP] 2) pair : [CLS] A [SEP] B [SEP]\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "    \n",
    "    def get_special_tokens_mask(self, token_ids_0, token_ids_1 = None,\n",
    "                               already_has_special_tokens=False):\n",
    "        \"\"\"token list에서 sequence ids로 변환(특수토큰을 추가하지 않고)\n",
    "        token_ids_0 : list of ids(특수 토큰을 포함하지 않은)\n",
    "        token_ids_1 : optional list of ids. sequence pairs\"\"\"\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of\"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "                return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, \n",
    "                               token_ids_0))\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "    \n",
    "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\" \n",
    "        sequence pair를 사용하여 두 시퀀스에서 mask를 만든다.BERT에서 pretraining 방식 중에 NSP 방식인것 같다.\n",
    "        첫번째 문장 토큰은 0 두번째 문장 토큰은 1.\n",
    "        만약에 두번째 문장이 존재하지 않는다면 0으로만 구성된 masking 방법도 적용가능\"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            return len(cls + token_ids_0 + sep) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + spe) * [1]\n",
    "    \n",
    "    def save_vocabulary(self, vocab_path):\n",
    "        \"\"\" tokenizer vocabulary 를 dictionary나 file로 저장.\"\"\"\n",
    "        index = 0\n",
    "        if os.path.isdir(vocab_path):\n",
    "            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "        else:\n",
    "            vocab_file = vocab_path\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                        \"Please check that the vocabulary is not corrupted!\".format(vocab_file) \n",
    "                        #format 함수 계속 까먹는다!!! 기억좀 해랏!\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "        retun (vocab_file,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e81906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordpieceTokenizer(object):\n",
    "    \"\"\" WordPiece tokenization\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"single 토큰, 공백 토큰 사용해서 단어를 작은 크기의 토큰으로 분리\"\"\"\n",
    "        \n",
    "        output_tokens = []\n",
    "        for token in whitespace_tokenize(text):\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word: # 최대길이를 넘으면 unk token 추가\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "                \n",
    "            is_bad = False\n",
    "            start = 0\n",
    "            sub_tokens = []\n",
    "            while start < len(chars):\n",
    "                end = len(chars)\n",
    "                cur_substr = None\n",
    "                while start < end:\n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    if start > 0:\n",
    "                        substr = \"##\" + substr\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                if cur_substr is None:\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end\n",
    "                \n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.append(sub_tokens)\n",
    "        return output_tokens\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "446f2e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_whitespace(char):\n",
    "    if char == \" \" or char ==\"\\t\" or char ==\"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "567f0498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_control(char):\n",
    "    if char ==\"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"C\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6931cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_punctuation(char):\n",
    "    cp = ord(char) #ord : 문자의 유니코드 함수 값 도출\n",
    "    if (cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85bfc1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerFast(PreTrainedTokenizerFast):\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        do_lower_case=True,\n",
    "        do_basic_tokenize=True,\n",
    "        never_split=None,\n",
    "        unk_token=\"[UNK]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        clean_text=True,\n",
    "        tokenize_chinese_chars=True,\n",
    "        add_special_tokens=True,\n",
    "        strip_accents=True,\n",
    "        wordpieces_prefix=\"##\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            BertWordPieceTokenizer(\n",
    "                vocab_file=vocab_file,\n",
    "                add_special_tokens=add_special_tokens,\n",
    "                unk_token=unk_token,\n",
    "                sep_token=sep_token,\n",
    "                cls_token=cls_token,\n",
    "                clean_text=clean_text,\n",
    "                handle_chinese_chars=tokenize_chinese_chars,\n",
    "                strip_accents=strip_accents,\n",
    "                lowercase=do_lower_case,\n",
    "                wordpieces_prefix=wordpieces_prefix,\n",
    "            ),\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.do_lower_case = do_lower_case\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        output = [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "\n",
    "        if token_ids_1:\n",
    "            output += token_ids_1 + [self.sep_token_id]\n",
    "\n",
    "        return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ddfe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
